# ğŸ”¥ NSFW Detector

A Python library and REST API for detecting NSFW (Not Safe For Work) content in images using TensorFlow/Keras.

<div align="center">

![Python](https://img.shields.io/badge/python-3.8+-blue.svg)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.13.0-orange.svg)
![Flask](https://img.shields.io/badge/Flask-2.3.3-green.svg)
![License](https://img.shields.io/badge/license-MIT-blue.svg)

</div>

## âœ¨ Features

- **ğŸ¯ 5-Class Detection**: drawings, hentai, neutral, porn, sexy
- **ğŸ“¸ Single Image Analysis**: Quickly analyze individual images
- **ğŸ“ Batch Processing**: Process multiple images simultaneously
- **ğŸ”„ Directory Scanning**: Automatically scan entire folders
- **ğŸŒ REST API**: HTTP endpoints for easy integration
- **ğŸ“Š NSFW Score**: Probability score from 0-1
- **âš™ï¸ Flexible Threshold**: Customizable NSFW threshold values

## ğŸš€ Quick Start

### Installation

```bash
pip install -r requirements.txt
```

### Model Setup

Place your H5 model file at `models/mobilenet_v2_140_224/saved_model.h5`

### Basic Usage

```python
from nsfw_detector import predict

# Load model
model = predict.load_model('models/mobilenet_v2_140_224/saved_model.h5')

# Single image prediction
result = predict.classify(model, 'image.jpg')
print(result)

# Get NSFW score
scores = result['image.jpg']
nsfw_score = predict.get_nsfw_score(scores)  # 0.148
is_nsfw = predict.is_nsfw(scores)  # False (threshold: 0.5)
```

## ğŸ“¡ API Server

### Start the Server

```bash
python api_server.py
```

Server runs on `http://localhost:5000`

### Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/` | API information |
| `GET` | `/health` | Health check |
| `GET` | `/stats` | Model statistics |
| `POST` | `/predict` | Upload file for prediction |
| `POST` | `/predict_url` | Predict from image URL |
| `POST` | `/predict_batch` | Batch prediction |

### Example Usage

**Upload File**
```bash
curl -X POST -F "file=@image.jpg" http://localhost:5000/predict
```

**Predict from URL**
```bash
curl -X POST -H "Content-Type: application/json" \
     -d '{"url":"https://example.com/image.jpg"}' \
     http://localhost:5000/predict_url
```

**Batch Processing**
```bash
curl -X POST -F "files=@img1.jpg" -F "files=@img2.jpg" \
     http://localhost:5000/predict_batch
```

## ğŸ“‹ API Response Format

```json
{
  "success": true,
  "filename": "image.jpg",
  "result": {
    "is_nsfw": false,
    "nsfw_score": 0.1481,
    "predicted_class": "drawings",
    "confidence": 0.8514,
    "scores": {
      "drawings": 0.85139805,
      "hentai": 0.14751932,
      "neutral": 0.00026579,
      "porn": 0.00077335,
      "sexy": 0.00004345
    }
  }
}
```

## ğŸ—ï¸ Project Structure

```
nsfw-detector/
â”œâ”€â”€ nsfw_detector.py        # Main detector module
â”œâ”€â”€ api_server.py           # Flask REST API
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ models/                 # Model files
â”‚   â””â”€â”€ mobilenet_v2_140_224/
â”‚       â””â”€â”€ saved_model.h5  # TensorFlow model
â””â”€â”€ README.md              # This file
```

## ğŸ¨ Classification Classes

| Class | Description | NSFW |
|-------|-------------|------|
| `drawings` | Drawings/animations | âŒ |
| `hentai` | Hentai content | âœ… |
| `neutral` | Neutral/normal content | âŒ |
| `porn` | Pornographic content | âœ… |
| `sexy` | Sexy/suggestive content | âœ… |

**NSFW Score**: Sum of `hentai + porn + sexy` scores

## âš™ï¸ Configuration

### Custom Threshold

```python
# More sensitive (detects more NSFW)
is_nsfw = predict.is_nsfw(scores, threshold=0.3)

# More tolerant (detects less NSFW)
is_nsfw = predict.is_nsfw(scores, threshold=0.8)
```

### Change API Port

```python
# In api_server.py
app.run(host='0.0.0.0', port=8080, debug=False)
```

## ğŸ”§ Development

### Testing

```bash
# Test the detector module
python nsfw_detector.py

# Start API in debug mode
python api_server.py
```

### Logging

```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

## ğŸ“Š Model Information

- **Architecture**: MobileNet V2 (140, 224)
- **Input Size**: 224x224 pixels
- **Classes**: 5 (drawings, hentai, neutral, porn, sexy)
- **Framework**: TensorFlow 2.13.0

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- TensorFlow team for the excellent ML framework
- MobileNet architecture for efficient mobile deployment
- Open source community for inspiration and tools

## âš ï¸ Important Notes

- This tool is designed for content moderation purposes only
- 100% accuracy is not guaranteed
- Human review is recommended for critical applications
- Model performance may vary based on image quality and content type

## ğŸ“ Support

For questions and support, please [open an issue](https://github.com/sw3do/nsfw-detector/issues) on GitHub.

---

<div align="center">
Made with â¤ï¸ by <a href="https://github.com/sw3do">sw3do</a>
</div> 